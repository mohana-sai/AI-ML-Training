{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>Author: [Yury Kashnitsky](https://www.linkedin.com/in/festline/), Data Scientist @ Mail.Ru Group <br>All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.","metadata":{"_uuid":"166053360ddfe2fa8ba8a2c15dcdda86eb2ef196"}},{"cell_type":"markdown","source":"# <center> Assignment #10 (demo)\n## <center> Gradient boosting\n\nYour task is to beat at least 2 benchmarks in this [Kaggle Inclass competition](https://www.kaggle.com/c/flight-delays-spring-2018). Here you won’t be provided with detailed instructions. We only give you a brief description of how the second benchmark was achieved using Xgboost. Hopefully, at this stage of the course, it's enough for you to take a quick look at the data in order to understand that this is the type of task where gradient boosting will perform well. Most likely it will be Xgboost, however, we’ve got plenty of categorical features here.\n\n<img src=https://habrastorage.org/webt/fs/42/ms/fs42ms0r7qsoj-da4x7yfntwrbq.jpeg width=40% />","metadata":{"_uuid":"045994ce90f88136eb95a5060fede8200f6d7bc6"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"54de4536192c459790ecb405b25a96b994aef43c","execution":{"iopub.status.busy":"2021-11-28T14:13:08.887907Z","iopub.execute_input":"2021-11-28T14:13:08.888501Z","iopub.status.idle":"2021-11-28T14:13:09.841745Z","shell.execute_reply.started":"2021-11-28T14:13:08.888448Z","shell.execute_reply":"2021-11-28T14:13:09.840927Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('../input/flight_delays_train.csv')\ntest = pd.read_csv('../input/flight_delays_test.csv')","metadata":{"_uuid":"d659e9a5cfd2ee5b982fa31cfb7b77109f9414f7","execution":{"iopub.status.busy":"2021-11-28T14:13:09.843006Z","iopub.execute_input":"2021-11-28T14:13:09.843311Z","iopub.status.idle":"2021-11-28T14:13:10.249606Z","shell.execute_reply.started":"2021-11-28T14:13:09.843263Z","shell.execute_reply":"2021-11-28T14:13:10.248776Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_uuid":"8b66b8dacaef0270f63afef365f295400491613b","execution":{"iopub.status.busy":"2021-11-28T14:13:10.250978Z","iopub.execute_input":"2021-11-28T14:13:10.251267Z","iopub.status.idle":"2021-11-28T14:13:10.300307Z","shell.execute_reply.started":"2021-11-28T14:13:10.251212Z","shell.execute_reply":"2021-11-28T14:13:10.299450Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"scrolled":true,"_uuid":"c9052adf73dff5f1ef4d2802f25a798eb46567e5","execution":{"iopub.status.busy":"2021-11-28T14:17:12.872811Z","iopub.execute_input":"2021-11-28T14:17:12.873140Z","iopub.status.idle":"2021-11-28T14:17:12.900303Z","shell.execute_reply.started":"2021-11-28T14:17:12.873081Z","shell.execute_reply":"2021-11-28T14:17:12.899402Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Given flight departure time, carrier's code, departure airport, destination location, and flight distance, you have to predict departure delay for more than 15 minutes. As the simplest benchmark, let's take Xgboost classifier and two features that are easiest to take: DepTime and Distance. Such model results in 0.68202 on the LB.","metadata":{"_uuid":"2c71792436f3718f22effd037013141867ad11cf"}},{"cell_type":"code","source":"X_train = train[['Distance', 'DepTime']].values\ny_train = train['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\nX_test = test[['Distance', 'DepTime']].values\n\nX_train_part, X_valid, y_train_part, y_valid = \\\n    train_test_split(X_train, y_train, \n                     test_size=0.3, random_state=17)","metadata":{"_uuid":"a2311aace61bb1be8982c4807e196fa5e8c2c75d","execution":{"iopub.status.busy":"2021-11-28T14:17:15.860740Z","iopub.execute_input":"2021-11-28T14:17:15.861206Z","iopub.status.idle":"2021-11-28T14:17:15.891355Z","shell.execute_reply.started":"2021-11-28T14:17:15.861163Z","shell.execute_reply":"2021-11-28T14:17:15.890679Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"We'll train Xgboost with default parameters on part of data and estimate holdout ROC AUC.","metadata":{"_uuid":"98f4566f7dfc7b2fcbde82d3b301ea651414c4a5"}},{"cell_type":"code","source":"xgb_model = XGBClassifier(seed=17)\n\nxgb_model.fit(X_train_part, y_train_part)\nxgb_valid_pred = xgb_model.predict_proba(X_valid)[:, 1]\n\nroc_auc_score(y_valid, xgb_valid_pred)","metadata":{"_uuid":"93185068b56d63058c7fa62160a1880945bbde1a","execution":{"iopub.status.busy":"2021-11-28T14:17:24.297293Z","iopub.execute_input":"2021-11-28T14:17:24.297883Z","iopub.status.idle":"2021-11-28T14:17:25.509037Z","shell.execute_reply.started":"2021-11-28T14:17:24.297832Z","shell.execute_reply":"2021-11-28T14:17:25.508187Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Now we do the same with the whole training set, make predictions to test set and form a submission file. This is how you beat the first benchmark. ","metadata":{"_uuid":"309de72eabfd6d0a3b5178e1c28a6b66a3d9a910"}},{"cell_type":"code","source":"xgb_model.fit(X_train, y_train)\nxgb_test_pred = xgb_model.predict_proba(X_test)[:, 1]\n\npd.Series(xgb_test_pred, \n          name='dep_delayed_15min').to_csv('xgb_2feat.csv', \n                                           index_label='id', header=True)","metadata":{"_uuid":"bdee9cabaa742f664da42ab239cd079f5221a523","execution":{"iopub.status.busy":"2021-11-28T14:17:26.946690Z","iopub.execute_input":"2021-11-28T14:17:26.947029Z","iopub.status.idle":"2021-11-28T14:17:29.462047Z","shell.execute_reply.started":"2021-11-28T14:17:26.946965Z","shell.execute_reply":"2021-11-28T14:17:29.461210Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The second benchmark in the leaderboard was achieved as follows:\n\n- Features `Distance` and `DepTime` were taken unchanged\n- A feature `Flight` was created from features `Origin` and `Dest`\n- Features `Month`, `DayofMonth`, `DayOfWeek`, `UniqueCarrier` and `Flight` were transformed with OHE (`LabelBinarizer`)\n- Logistic regression and gradient boosting (xgboost) were trained. Xgboost hyperparameters were tuned via cross-validation. First, the hyperparameters responsible for model complexity were optimized, then the number of trees was fixed at 500 and learning step was tuned.\n- Predicted probabilities were made via cross-validation using `cross_val_predict`. A linear mixture of logistic regression and gradient boosting predictions was set in the form $w_1 * p_{logit} + (1 - w_1) * p_{xgb}$, where $p_{logit}$ is a probability of class 1, predicted by logistic regression, and $p_{xgb}$ – the same for xgboost. $w_1$ weight was selected manually.\n- A similar combination of predictions was made for test set. \n\nFollowing the same steps is not mandatory. That’s just a description of how the result was achieved by the author of this assignment. Perhaps you might not want to follow the same steps, and instead, let’s say, add a couple of good features and train a random forest of a thousand trees.\n\nGood luck!","metadata":{"_uuid":"ae6ffbce03dca38bde48073269a5df0d89d1a71a"}}]}